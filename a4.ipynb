{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from statistics import mode\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(train_file):\n",
    "    file = open(train_file, 'r')\n",
    "\n",
    "    tweets = file.readlines()\n",
    "    tweets = [re.sub('\\n', '', tweet) for tweet in tweets]\n",
    "\n",
    "    targets = [tweet.split()[0] for tweet in tweets]\n",
    "    text = [tweet.lower().split()[1:] for tweet in tweets]\n",
    "\n",
    "    loc_word_dict = {}\n",
    "    for i in range(len(targets)):\n",
    "        for word in text[i]:\n",
    "            if targets[i] in loc_word_dict.keys():\n",
    "                if word in loc_word_dict[targets[i]].keys():\n",
    "                    loc_word_dict[targets[i]][word] += 1\n",
    "                else:\n",
    "                    loc_word_dict[targets[i]][word] = 1\n",
    "            else:\n",
    "                loc_word_dict[targets[i]] = {word: 1}\n",
    "\n",
    "    word_loc_dict = {}\n",
    "    for i in range(len(targets)):\n",
    "        for word in text[i]:\n",
    "            if word in word_loc_dict.keys():\n",
    "                if targets[i] in word_loc_dict[word].keys():\n",
    "                    word_loc_dict[word][targets[i]] += 1\n",
    "                else:\n",
    "                    word_loc_dict[word][targets[i]] = 1\n",
    "            else:\n",
    "                word_loc_dict[word] = {targets[i]: 1}\n",
    "\n",
    "    p_L = Counter(targets)\n",
    "\n",
    "    for value in loc_word_dict.values():\n",
    "        total = sum(value.values())\n",
    "        for key in value.keys():\n",
    "            value[key] = value[key] / total\n",
    "\n",
    "    word_loc_dict2 = {}\n",
    "    for key in word_loc_dict.keys():\n",
    "        if sum(word_loc_dict[key].values()) >= 5:\n",
    "            word_loc_dict2[key] = word_loc_dict[key]\n",
    "\n",
    "    for value in word_loc_dict2.values():\n",
    "        total = sum(value.values())\n",
    "        for key in value.keys():\n",
    "            value[key] = [value[key] / total, value[key]]\n",
    "    \n",
    "    total = sum(p_L.values())\n",
    "    for key in p_L.keys():\n",
    "        p_L[key] = p_L[key] / total\n",
    "\n",
    "    top_words = {}\n",
    "    for loc in set(targets):\n",
    "        words = []\n",
    "        for word in word_loc_dict2.keys():\n",
    "            if loc in word_loc_dict2[word].keys():\n",
    "                words.append([word, word_loc_dict2[word][loc]])\n",
    "        words = sorted(words, key = lambda x: (x[1][0], x[1][1]), reverse = True)[0:5]\n",
    "        words = [word[0] for word in words]\n",
    "        top_words[loc] = words\n",
    "\n",
    "    for key in top_words.keys():\n",
    "            ws = ', '.join(top_words[key])\n",
    "            out = f'The top 5 words for {key} are: {ws}'\n",
    "            print(out)\n",
    "    \n",
    "\n",
    "    return loc_word_dict, p_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_file_bayes(test_file):\n",
    "    file = open(test_file, 'r')\n",
    "\n",
    "    tweets = file.readlines()\n",
    "    tweets = [re.sub('\\n', '', tweet) for tweet in tweets]\n",
    "\n",
    "    targets = [tweet.split()[0] for tweet in tweets]\n",
    "    text = [' '.join(tweet.split()[1:]) for tweet in tweets]\n",
    "\n",
    "    return (targets, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_target(test_tweet, target, loc_word_dict, p_L):\n",
    "    tokenized_tweet = test_tweet.lower().split()\n",
    "    score = math.log(p_L[target])\n",
    "    for token in tokenized_tweet:\n",
    "        if token in loc_word_dict[target].keys():\n",
    "            score += math.log(loc_word_dict[target][token])\n",
    "        else:\n",
    "            score += math.log(1 / 100000)\n",
    "    return (target, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_test(test_tweet, targets, loc_word_dict, p_L):\n",
    "    best_score = -1000000000000000\n",
    "    best_target = ''\n",
    "    for loc in targets:\n",
    "        pos_target, pos_score = test_one_target(test_tweet, loc, loc_word_dict, p_L)\n",
    "        if pos_score > best_score:\n",
    "            best_score = pos_score\n",
    "            best_target = pos_target\n",
    "    return best_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The top 5 words for Philadelphia,_PA are: philadelphia,, #philadelphia,, pa), philadelphia, phillies\nThe top 5 words for San_Francisco,_CA are: francisco,, #sanfrancisco,, (#sanfrancisco,, #sf, fran\nThe top 5 words for Orlando,_FL are: #orlpol, #opd, #orlando,, fl, orlando,\nThe top 5 words for Manhattan,_NY are: ny), #newyork,, (#newyork,, ny?, cleared:\nThe top 5 words for Los_Angeles,_CA are: angeles,, #losangeles,, dodger, (#losangeles,, #dodgers\nThe top 5 words for Chicago,_IL are: chicago,, #chicago,, illinois, (#chicago,, il?\nThe top 5 words for Atlanta,_GA are: #atlanta,, atlanta,, georgia, (#atlanta,, ga?\nThe top 5 words for Toronto,_Ontario are: toronto,, trucks), #toronto, #toronto,, b/w\nThe top 5 words for Boston,_MA are: #boston,, ma), massachusetts, ma?, (#boston,\nThe top 5 words for Houston,_TX are: #houston,, houston,, tx), (#houston,, beds,\nThe top 5 words for Washington,_DC are: washington,, #washington,, dc), d.c., (#washington,\nThe top 5 words for San_Diego,_CA are: diego,, (#sandiego,, petco, jolla, #seaworld\n"
     ]
    }
   ],
   "source": [
    "loc_word_dict, p_L = train_bayes('tweets.train.clean.txt')"
   ]
  },
  {
   "source": [
    "test_targets, test_text = read_train_file('tweets.test1.clean.txt')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_bayes(test_text, test_targets, loc_word_dict, p_L, output_file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    for i in range(len(test_text)):\n",
    "        prediction = bayes_test(test_text[i], set(test_targets), loc_word_dict, p_L)\n",
    "        predictions.append((prediction, test_targets[i], test_text[i]))\n",
    "        total += 1\n",
    "        if prediction == test_targets[i]:\n",
    "            correct += 1\n",
    "\n",
    "    f = open(output_file, \"w\")\n",
    "    for line in predictions:\n",
    "        for word in line:\n",
    "            f.write(word)\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    score = correct / total\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.632\n"
     ]
    }
   ],
   "source": [
    "predict_bayes(test_text, test_targets, loc_word_dict, p_L, 'bayes_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_file_dtree(train_file):\n",
    "    file = open(train_file, 'r')\n",
    "\n",
    "    tweets = file.readlines()\n",
    "    tweets = [re.sub('\\n', '', tweet) for tweet in tweets]\n",
    "\n",
    "    targets = [tweet.split()[0] for tweet in tweets]\n",
    "    text = [' '.join(tweet.split()[1:]) for tweet in tweets]\n",
    "    text2 = [re.findall(r'\\w\\w+', tweet.lower())[1:] for tweet in tweets]\n",
    "\n",
    "    word_loc_dict = {}\n",
    "    for i in range(len(targets)):\n",
    "        for word in text2[i]:\n",
    "            if word in word_loc_dict.keys():\n",
    "                if targets[i] in word_loc_dict[word].keys():\n",
    "                    word_loc_dict[word][targets[i]] += 1\n",
    "                else:\n",
    "                    word_loc_dict[word][targets[i]] = 1\n",
    "            else:\n",
    "                word_loc_dict[word] = {targets[i]: 1}\n",
    "\n",
    "    word_loc_dict2 = {}\n",
    "    for key in word_loc_dict.keys():\n",
    "        if sum(word_loc_dict[key].values()) >= 20:\n",
    "            word_loc_dict2[key] = word_loc_dict[key]\n",
    "\n",
    "    top_words = []\n",
    "    for loc in set(targets):\n",
    "        words = []\n",
    "        for word in word_loc_dict2.keys():\n",
    "            if loc in word_loc_dict2[word].keys():\n",
    "                words.append([word, word_loc_dict2[word][loc]])\n",
    "        words = sorted(words, key = lambda z: z[1], reverse = True)[0:100]\n",
    "        words = [word[0] for word in words]\n",
    "        top_words += words\n",
    "\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    X = cv.fit_transform(text)\n",
    "\n",
    "    top_words = sorted([cv.get_feature_names().index(word) for word in set(top_words) if word in cv.get_feature_names()])\n",
    "\n",
    "    X = X.toarray()[:, top_words]\n",
    "    word_list = [cv.get_feature_names()[index] for index in top_words]\n",
    "\n",
    "\n",
    "    return (X, np.array(targets), word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dtree(matrix, labels, word_list, dtree, min_leaves, max_depth, depth = 1):\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels[0]\n",
    "    elif len(labels) <= min_leaves:\n",
    "        return mode(labels)\n",
    "    elif depth == max_depth:\n",
    "        return mode(labels)\n",
    "    else:\n",
    "        split_word, split_word_index = best_split(matrix, labels, word_list)\n",
    "        dtree['node'] = split_word\n",
    "        attr = matrix[:,split_word_index]\n",
    "\n",
    "        left_filter = [attr == 1]\n",
    "        right_filter = [attr == 0]\n",
    "\n",
    "        left_matrix = np.delete(matrix[left_filter], split_word_index, axis = 1)\n",
    "        left_labels = labels[left_filter]\n",
    "\n",
    "        right_matrix = np.delete(matrix[right_filter], split_word_index, axis = 1)\n",
    "        right_labels = labels[right_filter]\n",
    "\n",
    "        word_list.pop(split_word_index)\n",
    "\n",
    "        empty_dict_left = {'node': '', 'left': {}, 'right': {}}\n",
    "        empty_dict_right = {'node': '', 'left': {}, 'right': {}}\n",
    "\n",
    "        dtree['left'] = train_dtree(left_matrix, left_labels, word_list, empty_dict_left, min_leaves, max_depth, depth+1)\n",
    "        dtree['right'] = train_dtree(right_matrix, right_labels, word_list, empty_dict_right, min_leaves, max_depth, depth+1)\n",
    "\n",
    "    return dtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tree = {'node': '', 'left': {}, 'right': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 30.1 s, sys: 4.51 s, total: 34.6 s\nWall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matrix, labels, word_list= read_train_file_dtree('tweets.train.clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 4min 11s, sys: 1.49 s, total: 4min 12s\nWall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mat = train_dtree(matrix, labels, word_list, default_tree, 10, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'node': 'chicago',\n",
       " 'left': {'node': 'of',\n",
       "  'left': {'node': 'philly',\n",
       "   'left': 'Philadelphia,_PA',\n",
       "   'right': {'node': 'california',\n",
       "    'left': 'San_Diego,_CA',\n",
       "    'right': {'node': 'up', 'left': 'San_Diego,_CA', 'right': 'Chicago,_IL'}}},\n",
       "  'right': {'node': 'hughey',\n",
       "   'left': 'Houston,_TX',\n",
       "   'right': {'node': 'registered',\n",
       "    'left': 'Chicago,_IL',\n",
       "    'right': {'node': 'night',\n",
       "     'left': 'Manhattan,_NY',\n",
       "     'right': {'node': 'back',\n",
       "      'left': {'node': 'click',\n",
       "       'left': 'Toronto,_Ontario',\n",
       "       'right': 'Chicago,_IL'},\n",
       "      'right': {'node': 'by',\n",
       "       'left': {'node': '______',\n",
       "        'left': 'Los_Angeles,_CA',\n",
       "        'right': 'Chicago,_IL'},\n",
       "       'right': {'node': 'arena',\n",
       "        'left': 'Chicago,_IL',\n",
       "        'right': {'node': 'not',\n",
       "         'left': 'Chicago,_IL',\n",
       "         'right': 'Chicago,_IL'}}}}}}}},\n",
       " 'right': {'node': 'in',\n",
       "  'left': {'node': 'today',\n",
       "   'left': 'Manhattan,_NY',\n",
       "   'right': {'node': 'opened',\n",
       "    'left': 'Manhattan,_NY',\n",
       "    'right': {'node': 'want',\n",
       "     'left': 'Houston,_TX',\n",
       "     'right': {'node': 'trespasser',\n",
       "      'left': {'node': 'lajolla',\n",
       "       'left': 'Houston,_TX',\n",
       "       'right': {'node': 'trucks',\n",
       "        'left': 'Houston,_TX',\n",
       "        'right': {'node': '__',\n",
       "         'left': 'Houston,_TX',\n",
       "         'right': 'Houston,_TX'}}},\n",
       "      'right': {'node': 'united',\n",
       "       'left': 'Houston,_TX',\n",
       "       'right': {'node': 'jobs',\n",
       "        'left': 'Houston,_TX',\n",
       "        'right': {'node': 'read',\n",
       "         'left': 'Houston,_TX',\n",
       "         'right': 'Houston,_TX'}}}}}}},\n",
       "  'right': {'node': 'careerarc',\n",
       "   'left': {'node': 'street',\n",
       "    'left': 'San_Francisco,_CA',\n",
       "    'right': {'node': 'game',\n",
       "     'left': {'node': 'medical',\n",
       "      'left': 'Los_Angeles,_CA',\n",
       "      'right': 'San_Francisco,_CA'},\n",
       "     'right': {'node': 'studios',\n",
       "      'left': 'San_Diego,_CA',\n",
       "      'right': {'node': 'downtown',\n",
       "       'left': 'San_Diego,_CA',\n",
       "       'right': {'node': 'ma',\n",
       "        'left': 'San_Diego,_CA',\n",
       "        'right': {'node': 'we',\n",
       "         'left': 'Toronto,_Ontario',\n",
       "         'right': 'Los_Angeles,_CA'}}}}}},\n",
       "   'right': {'node': 'buckhead',\n",
       "    'left': {'node': 'museum',\n",
       "     'left': {'node': 'ud', 'left': 'Manhattan,_NY', 'right': 'Boston,_MA'},\n",
       "     'right': {'node': 'washingtondc',\n",
       "      'left': 'Boston,_MA',\n",
       "      'right': {'node': 'care',\n",
       "       'left': {'node': '15', 'left': 'Washington,_DC', 'right': 'Boston,_MA'},\n",
       "       'right': {'node': 'road',\n",
       "        'left': 'Washington,_DC',\n",
       "        'right': {'node': 'scaa2016',\n",
       "         'left': 'Philadelphia,_PA',\n",
       "         'right': 'Boston,_MA'}}}}},\n",
       "    'right': {'node': 'bank',\n",
       "     'left': {'node': 'hiring',\n",
       "      'left': 'Atlanta,_GA',\n",
       "      'right': {'node': 'harlem',\n",
       "       'left': 'Atlanta,_GA',\n",
       "       'right': {'node': 'ave',\n",
       "        'left': {'node': 'alert',\n",
       "         'left': 'Los_Angeles,_CA',\n",
       "         'right': 'Atlanta,_GA'},\n",
       "        'right': {'node': 'with',\n",
       "         'left': 'Atlanta,_GA',\n",
       "         'right': 'Atlanta,_GA'}}}},\n",
       "     'right': {'node': 'pennsylvania',\n",
       "      'left': {'node': 'memorial',\n",
       "       'left': 'Manhattan,_NY',\n",
       "       'right': {'node': 'domesticdisturbance',\n",
       "        'left': 'Manhattan,_NY',\n",
       "        'right': {'node': '000',\n",
       "         'left': 'San_Diego,_CA',\n",
       "         'right': 'Manhattan,_NY'}}},\n",
       "      'right': {'node': 'sales',\n",
       "       'left': 'Orlando,_FL',\n",
       "       'right': {'node': 'rn',\n",
       "        'left': {'node': 'graffiti',\n",
       "         'left': 'Orlando,_FL',\n",
       "         'right': 'Orlando,_FL'},\n",
       "        'right': {'node': 'spring',\n",
       "         'left': 'Philadelphia,_PA',\n",
       "         'right': 'Manhattan,_NY'}}}}}}}}}"
      ]
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(attr, labels):\n",
    "    ones = labels[attr == 1]\n",
    "    zeroes = labels[attr == 0]\n",
    "    one_length = len(ones)\n",
    "    zero_length = len(zeroes)\n",
    "\n",
    "    ones_entropy = 0\n",
    "    for label in set(labels):\n",
    "        label_count = sum([1 for word in ones if word == label])\n",
    "        if one_length == 0:\n",
    "            prop = 0\n",
    "        else:\n",
    "            prop = label_count / one_length\n",
    "        ones_entropy += 0 if prop == 0 else (-prop * math.log(prop, 2))\n",
    "\n",
    "    zeroes_entropy = 0\n",
    "    for label in set(labels):\n",
    "        label_count = sum([1 for word in zeroes if word == label])\n",
    "        if zero_length == 0:\n",
    "            prop = 0\n",
    "        else:\n",
    "            prop = label_count / zero_length\n",
    "        zeroes_entropy += 0 if prop == 0 else (-prop * math.log(prop, 2))\n",
    "\n",
    "    total_entropy = ((ones_entropy * one_length) + (zeroes_entropy * zero_length)) / (one_length + zero_length)\n",
    "    \n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(matrix, labels, word_list):\n",
    "    best_choice = ''\n",
    "    best_entropy = 1000000000000\n",
    "    for i in range(len(word_list)):\n",
    "        attr = matrix[:,i]\n",
    "        ent = entropy(attr, labels)\n",
    "        if ent < best_entropy:\n",
    "            best_entropy = ent\n",
    "            best_choice = word_list[i]\n",
    "            best_choice_index = i\n",
    "    return (best_choice, best_choice_index)\n"
   ]
  }
 ]
}