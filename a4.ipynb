{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bayes(train_file):\n",
    "    file = open(train_file, 'r')\n",
    "\n",
    "    tweets = file.readlines()\n",
    "    tweets = [re.sub('\\n', '', tweet) for tweet in tweets]\n",
    "\n",
    "    targets = [tweet.split()[0] for tweet in tweets]\n",
    "    text = [' '.join(tweet.split()[1:]) for tweet in tweets]\n",
    "\n",
    "    # learn the counts of locations\n",
    "    p_L = collections.Counter(targets)\n",
    "\n",
    "    # learn counts for each word by location\n",
    "    cv = CountVectorizer()\n",
    "    X = cv.fit_transform(text)\n",
    "    word_df = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())\n",
    "    word_df['loc_target_Neelan'] = targets\n",
    "    word_df = word_df.groupby(by = 'loc_target_Neelan').sum()\n",
    "    word_df['loc_counts_Neelan'] = [p_L[key] for key in sorted(p_L.keys())]\n",
    "\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_df = train_df.loc[:, (train_df.sum(axis=0) > 5)].astype('double')\n",
    "top_word_df.drop('loc_counts_Neelan', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Washington,_DC',\n",
       "  ['amend',\n",
       "   'churchkeydc',\n",
       "   'ecd',\n",
       "   'igdc',\n",
       "   'kidsfirst',\n",
       "   'mguh',\n",
       "   'momofuku',\n",
       "   'scifest',\n",
       "   'sibley',\n",
       "   'smithsonian',\n",
       "   'truckeroo',\n",
       "   'washingtondc',\n",
       "   'whitehouse',\n",
       "   'wpgc',\n",
       "   'dc',\n",
       "   'capitol',\n",
       "   'fbi',\n",
       "   'washington',\n",
       "   'georgetown',\n",
       "   'nationals']),\n",
       " ('Orlando,_FL',\n",
       "  ['32801',\n",
       "   '32803',\n",
       "   '32804',\n",
       "   '32805',\n",
       "   '32814',\n",
       "   '911hangup',\n",
       "   'accidentinjuries',\n",
       "   'accidentminor',\n",
       "   'accidentwithroadblockage',\n",
       "   'amelia',\n",
       "   'amway',\n",
       "   'attemptedsuicide',\n",
       "   'baldwinpark',\n",
       "   'bumby',\n",
       "   'centralbusinessdistrict',\n",
       "   'collegepark',\n",
       "   'colonial',\n",
       "   'commercialalarm',\n",
       "   'conroy',\n",
       "   'designatedpatrolareaavailable']),\n",
       " ('Boston,_MA',\n",
       "  ['allston',\n",
       "   'berklee',\n",
       "   'bidmc',\n",
       "   'bombings',\n",
       "   'bos',\n",
       "   'bostonmarathon2016',\n",
       "   'dorchester',\n",
       "   'eastboston',\n",
       "   'fenway',\n",
       "   'fenwaypark',\n",
       "   'mwen',\n",
       "   'roslindale',\n",
       "   'roxbury',\n",
       "   'massachusetts',\n",
       "   'onebostonday',\n",
       "   'boston',\n",
       "   'bostonstrong',\n",
       "   'illegal',\n",
       "   'redsox',\n",
       "   'ma']),\n",
       " ('Toronto,_Ontario',\n",
       "  ['6ix',\n",
       "   'aircanadacentre',\n",
       "   'barometer',\n",
       "   'bathurst',\n",
       "   'bloor',\n",
       "   'cafa',\n",
       "   'cafa2016',\n",
       "   'cafawards',\n",
       "   'canadian',\n",
       "   'carbon',\n",
       "   'cibc',\n",
       "   'cibcjobs',\n",
       "   'cntower',\n",
       "   'convulsions',\n",
       "   'crt',\n",
       "   'danforth',\n",
       "   'dundas',\n",
       "   'edgeworxwanop',\n",
       "   'eglinton',\n",
       "   'etobicoke']),\n",
       " ('Atlanta,_GA',\n",
       "  ['285',\n",
       "   'atlfreestuff',\n",
       "   'atltraffic',\n",
       "   'bedowntown',\n",
       "   'buckhead',\n",
       "   'duran',\n",
       "   'duranduran',\n",
       "   'edgewood',\n",
       "   'eq',\n",
       "   'georgia',\n",
       "   'gwcc',\n",
       "   'gwcc_atl',\n",
       "   'jpdesignsart',\n",
       "   'moodsmusic',\n",
       "   'nelzzzzz32',\n",
       "   'oldkamals21',\n",
       "   'peachtree',\n",
       "   'piedmont',\n",
       "   'ponce',\n",
       "   'rakim']),\n",
       " ('San_Diego,_CA',\n",
       "  ['brytonejames',\n",
       "   'earlsbottomlip',\n",
       "   'gaslamp',\n",
       "   'gulls',\n",
       "   'hevon',\n",
       "   'jolla',\n",
       "   'lajolla',\n",
       "   'mikehessbrewing',\n",
       "   'mikkellersd',\n",
       "   'nofx',\n",
       "   'pacificbeach',\n",
       "   'parq',\n",
       "   'parqsd',\n",
       "   'sdinhd',\n",
       "   'seaworld',\n",
       "   'sandiego',\n",
       "   'diego',\n",
       "   'petco',\n",
       "   'sd',\n",
       "   'padres']),\n",
       " ('Manhattan,_NY',\n",
       "  ['125th',\n",
       "   '145th',\n",
       "   '212',\n",
       "   '34th',\n",
       "   '3line',\n",
       "   '42nd',\n",
       "   '59th',\n",
       "   'actus',\n",
       "   'afterwork',\n",
       "   'aipad',\n",
       "   'aoki',\n",
       "   'aprilfools',\n",
       "   'artexponewyork',\n",
       "   'authority',\n",
       "   'avantasia',\n",
       "   'bedford',\n",
       "   'billyjoel',\n",
       "   'biraattribeca',\n",
       "   'birainnyc',\n",
       "   'bmcc']),\n",
       " ('San_Francisco,_CA',\n",
       "  ['alcatraz',\n",
       "   'asiasf',\n",
       "   'davies',\n",
       "   'defects',\n",
       "   'dolores',\n",
       "   'encampment',\n",
       "   'facetstudio',\n",
       "   'fisherman',\n",
       "   'fishermans',\n",
       "   'jacku',\n",
       "   'lombard',\n",
       "   'miikesnow',\n",
       "   'noriega',\n",
       "   'potrero',\n",
       "   'sanfranciscoii',\n",
       "   'sanfranciscotrip',\n",
       "   'sfo',\n",
       "   'skye',\n",
       "   'warfield',\n",
       "   'sanfrancisco']),\n",
       " ('Philadelphia,_PA',\n",
       "  ['allegheny',\n",
       "   'breweryommegang',\n",
       "   'daddienotch',\n",
       "   'drexel',\n",
       "   'fishtown',\n",
       "   'independence',\n",
       "   'oldcity',\n",
       "   'phl',\n",
       "   'powerpost____',\n",
       "   'rittenhouse',\n",
       "   'septa',\n",
       "   'septaphilly',\n",
       "   'tla',\n",
       "   'tootsie',\n",
       "   'tri215',\n",
       "   'philadelphia',\n",
       "   'phillies',\n",
       "   'pa',\n",
       "   'pennsylvania',\n",
       "   'philly']),\n",
       " ('Los_Angeles,_CA',\n",
       "  ['10sm',\n",
       "   '405',\n",
       "   '41715',\n",
       "   'ao2',\n",
       "   'baltar',\n",
       "   'beatsf',\n",
       "   'belasco',\n",
       "   'brea',\n",
       "   'brentwood',\n",
       "   'brianmcknightjr',\n",
       "   'bsn',\n",
       "   'bzfilmz',\n",
       "   'californa',\n",
       "   'canogapark',\n",
       "   'careabouttheshot',\n",
       "   'careers',\n",
       "   'centurycity',\n",
       "   'chatsworth',\n",
       "   'clr',\n",
       "   'crenshaw']),\n",
       " ('Chicago,_IL',\n",
       "  ['blackhawks',\n",
       "   'careersattu',\n",
       "   'chicagoriver',\n",
       "   'chitown',\n",
       "   'cimmfest',\n",
       "   'cta',\n",
       "   'daley',\n",
       "   'depaul',\n",
       "   'fatherjohnmisty',\n",
       "   'foodiechats',\n",
       "   'garth',\n",
       "   'gocubsgo',\n",
       "   'godbox',\n",
       "   'hawks',\n",
       "   'infosec',\n",
       "   'jarvis',\n",
       "   'lifeattu',\n",
       "   'localh',\n",
       "   'lyric',\n",
       "   'metrochicago']),\n",
       " ('Houston,_TX',\n",
       "  ['610',\n",
       "   '77004',\n",
       "   '77006',\n",
       "   '77008',\n",
       "   '77057',\n",
       "   '77095',\n",
       "   '77346',\n",
       "   '77396',\n",
       "   'addicted2thehustle',\n",
       "   'ads2020',\n",
       "   'amonamarth',\n",
       "   'astros',\n",
       "   'baths',\n",
       "   'bbva',\n",
       "   'bbvacompassstdm',\n",
       "   'bossstatus',\n",
       "   'bpms150',\n",
       "   'dynamo',\n",
       "   'foreverorange',\n",
       "   'greenwayupperkirby'])]"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "top_words = []\n",
    "\n",
    "for token in top_word_df.columns:\n",
    "    total_words = top_word_df[token].sum()\n",
    "    top_word_df[token] = top_word_df[token] / total_words\n",
    "\n",
    "for loc in set(targets):\n",
    "    top_words.append((loc, [i for i in top_word_df.loc[loc].nlargest(20).index]))\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in top_word_df.loc['Atlanta,_GA'].nlargest(5).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['285', 'atlfreestuff', 'atltraffic', 'bedowntown', 'buckhead']"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   00  000  00000kt  000mph  001  003  007  008  00am  00pm  \\\n",
       "loc_target_Neelan                                                             \n",
       "Atlanta,_GA         2    1        0       0    0    1    1    0     1     0   \n",
       "Boston,_MA          1    3        0       0    0    0    0    0     0     0   \n",
       "Chicago,_IL         2    3        0       0    0    0    0    0     0     0   \n",
       "Houston,_TX         0   49        0       0    0    0    0    0     1     2   \n",
       "Los_Angeles,_CA     0    6        4       0    0    0    0    0     0     0   \n",
       "Manhattan,_NY       2    8        0       0    1    0    0    1     0     1   \n",
       "Orlando,_FL         8    0        0       0    0    0    0    0     0     0   \n",
       "Philadelphia,_PA    6    0        0       0    0    0    0    0     0     3   \n",
       "San_Diego,_CA       1    1        0       1    1    0    0    0     0     1   \n",
       "San_Francisco,_CA   0    1        0       0    0    0    0    0     0     0   \n",
       "Toronto,_Ontario    0    2        0       0    0    0    0    0     0     0   \n",
       "Washington,_DC      0    1        0       0    0    0    0    0     0     0   \n",
       "\n",
       "                   ...  zume  zuni  zuniga  zurichseeconnections  zwillinger  \\\n",
       "loc_target_Neelan  ...                                                         \n",
       "Atlanta,_GA        ...     0     0       0                     0           0   \n",
       "Boston,_MA         ...     1     0       0                     0           0   \n",
       "Chicago,_IL        ...     0     0       0                     0           0   \n",
       "Houston,_TX        ...     0     0       0                     0           0   \n",
       "Los_Angeles,_CA    ...     0     0       1                     0           0   \n",
       "Manhattan,_NY      ...     0     0       0                     1           1   \n",
       "Orlando,_FL        ...     0     0       0                     0           0   \n",
       "Philadelphia,_PA   ...     0     0       0                     0           0   \n",
       "San_Diego,_CA      ...     0     0       0                     0           0   \n",
       "San_Francisco,_CA  ...     0     1       0                     0           0   \n",
       "Toronto,_Ontario   ...     0     0       0                     0           0   \n",
       "Washington,_DC     ...     0     0       0                     0           0   \n",
       "\n",
       "                   zz  zzboy  zzztop  zzzzzzz  loc_counts_Neelan  \n",
       "loc_target_Neelan                                                 \n",
       "Atlanta,_GA         0      0       0        0               1910  \n",
       "Boston,_MA          0      0       0        0               1465  \n",
       "Chicago,_IL         0      0       0        0               3020  \n",
       "Houston,_TX         0      1       0        0               2461  \n",
       "Los_Angeles,_CA     0      0       1        0               5988  \n",
       "Manhattan,_NY       0      0       0        0               6787  \n",
       "Orlando,_FL         0      0       0        0               1401  \n",
       "Philadelphia,_PA    1      0       0        0               1663  \n",
       "San_Diego,_CA       0      0       0        0               1568  \n",
       "San_Francisco,_CA   0      0       0        0               2103  \n",
       "Toronto,_Ontario    0      0       0        0               1890  \n",
       "Washington,_DC      0      0       0        1               1744  \n",
       "\n",
       "[12 rows x 46471 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>00000kt</th>\n      <th>000mph</th>\n      <th>001</th>\n      <th>003</th>\n      <th>007</th>\n      <th>008</th>\n      <th>00am</th>\n      <th>00pm</th>\n      <th>...</th>\n      <th>zume</th>\n      <th>zuni</th>\n      <th>zuniga</th>\n      <th>zurichseeconnections</th>\n      <th>zwillinger</th>\n      <th>zz</th>\n      <th>zzboy</th>\n      <th>zzztop</th>\n      <th>zzzzzzz</th>\n      <th>loc_counts_Neelan</th>\n    </tr>\n    <tr>\n      <th>loc_target_Neelan</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Atlanta,_GA</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1910</td>\n    </tr>\n    <tr>\n      <td>Boston,_MA</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1465</td>\n    </tr>\n    <tr>\n      <td>Chicago,_IL</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3020</td>\n    </tr>\n    <tr>\n      <td>Houston,_TX</td>\n      <td>0</td>\n      <td>49</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2461</td>\n    </tr>\n    <tr>\n      <td>Los_Angeles,_CA</td>\n      <td>0</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5988</td>\n    </tr>\n    <tr>\n      <td>Manhattan,_NY</td>\n      <td>2</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6787</td>\n    </tr>\n    <tr>\n      <td>Orlando,_FL</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1401</td>\n    </tr>\n    <tr>\n      <td>Philadelphia,_PA</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1663</td>\n    </tr>\n    <tr>\n      <td>San_Diego,_CA</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1568</td>\n    </tr>\n    <tr>\n      <td>San_Francisco,_CA</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2103</td>\n    </tr>\n    <tr>\n      <td>Toronto,_Ontario</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1890</td>\n    </tr>\n    <tr>\n      <td>Washington,_DC</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1744</td>\n    </tr>\n  </tbody>\n</table>\n<p>12 rows × 46471 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_file(test_file):\n",
    "    file = open(test_file, 'r')\n",
    "\n",
    "    tweets = file.readlines()\n",
    "    tweets = [re.sub('\\n', '', tweet) for tweet in tweets]\n",
    "\n",
    "    targets = [tweet.split()[0] for tweet in tweets]\n",
    "    text = [' '.join(tweet.split()[1:]) for tweet in tweets]\n",
    "\n",
    "    return (targets, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_one_target(test_tweet, target, train_df):\n",
    "    test_tweet = test_tweet.lower()\n",
    "    tokenized_tweet = re.split(r'\\W', test_tweet)\n",
    "    score = math.log(train_df.loc[target]['loc_counts_Neelan'] / train_df['loc_counts_Neelan'].sum())\n",
    "    for token in tokenized_tweet:\n",
    "        if token in train_df.columns:\n",
    "            score += math.log((train_df.loc[target][token] + 1) / (train_df.loc[target].sum() + 1))\n",
    "        else:\n",
    "            score += math.log(1 / (train_df.loc[target].sum() + 1))\n",
    "    return (target, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_test(test_tweet, targets, train_df):\n",
    "    best_score = -1000000000000000\n",
    "    best_target = ''\n",
    "    for loc in targets:\n",
    "        pos_target, pos_score = test_one_target(test_tweet, loc, train_df)\n",
    "        if pos_score > best_score:\n",
    "            best_score = pos_score\n",
    "            best_target = pos_target\n",
    "    return best_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_bayes('tweets.train.clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets, test_text = read_test_file('tweets.test1.clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Washington,_DC'"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "bayes_test('Finally. @ Hamilton: An American Musical ', set(test_targets), train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}