# a4

## Part 1: Naive Bayes
For the Naive Bayes I started by learning a series of dictionaries. One had all of the words for each tweet location and their counts. I used this to calculate P(word|Location). Another dictionary had the counts of each location among the test tweets. I used this for P(Location). These two were the core dictionaries for the Naive Bayes model. When it came to making predictions I cycled through each location and calculated P(word1|L) * P(word2|L) * ... * P(L). Since P(D) would be the same for each location, there was no reason to calculate that since it would just be divded into each probability. Thus the prediction picked the location that maximized the products of the probabilities. 

The other dictionaries I created were to help me figure out the 5 best words to predict each location. First I created a dictionary with the different locations that each word appeared in and the number of times. Then, I got rid of any words that did not appear at least 5 times. Finally, for each location I picked the 5 words that had the highest proportion of their occurrences in that location. If there were ties, I chose the 5 words that had the highest number of occurrences. 

I didn't have too many problems with this section. One thing that I would do differently if I had more time, would be to try different tokenization methods. I simply split the tweets using str.split(). This led to many differnt tokens that were clearly not words. It's possible that this would lead to overfitting the training data and struggling on unseen data.

## Part 2: Deceision Tree
For the decision tree I began with an empty dictionary with a three keys, Node, Left, and Right. I first populated the node value with the word that ended up producing the lowest entropy for the feture matrix. Then I split the matrix into tweets the had the word and tweets that did not. I then recurrsively ran the learning method again but with the two newly created halves. I continued to do this until either a max depth had been reached, the data was perfectly split, or there were a minimium number of labels in a leaf node. 

This section was much more difficult and it is apparent that my implementation is not nearly as effective as the Naive Bayes algorithm since the accuracy score is so much worse. One of the biggest issues was with the amount of time it took to train the model. To remain under the 10 minute time limit I could not go very deep in the tree. To help speed things up, I tried to pair down the number of words I was using to build out the tree. I borrowed ideas from the Naive Bayes section and tried to use only the words that were most predictive of certain locations. I ended up choosing the top 50 words for each location and combined them into my final set of words that I passed through CountVectorizer. While this did help make things run faster, it did not end up having great results in terms of accuracy. A deeper tree or a better choice of words might be able to increase the accuracy, but I ran out of time.
